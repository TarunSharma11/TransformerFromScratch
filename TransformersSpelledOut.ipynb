{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTqvTvMG1_2f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "xaYGuaY-2Tqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iter = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "embed_size = 384\n",
        "num_heads = 6\n",
        "n_layers = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "qFd5-CUP2T0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self, inpt_size):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(inpt_size, inpt_size*4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(inpt_size*4, inpt_size),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "vK5OkjgL2ISZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size, masked = False, cross_attention = False):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(embed_size, head_size, bias = False)\n",
        "    self.query = nn.Linear(embed_size, head_size, bias = False)\n",
        "    self.value = nn.Linear(embed_size, head_size, bias = False)\n",
        "    self.masked = masked\n",
        "    self.cross_attention = cross_attention\n",
        "    if (self.masked):\n",
        "      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc = None):\n",
        "    B,T,C = x.shape\n",
        "    q = self.query(x)\n",
        "    if(self.cross_attention):\n",
        "      assert enc is not None\n",
        "      k = self.key(enc)\n",
        "      v = self.value(enc)\n",
        "    else:\n",
        "      k = self.key(x)\n",
        "      v = self.value(x)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    if (self.masked):\n",
        "      wei = wei.masked_fill(self.tril, float('-inf'))\n",
        "    wei = F.softmax(wei, dim = -1)\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "OLR-zAby25oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size, masked = False, cross_attention = False):\n",
        "    super().__init__()\n",
        "    self.head_list = nn.ModuleList([Head(head_size, masked, cross_attention) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(embed_size, embed_size)\n",
        "    self.dr = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    head_out = torch.cat([h(idx) for h in self.head_list], dim = -1)\n",
        "    out = self.proj(head_out)\n",
        "    out = self.dr(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "PMn6AUsw2TA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embed_size, num_heads):\n",
        "    super().__init__()\n",
        "    head_size = embed_size//num_heads\n",
        "    self.self_attn = MultiheadAttention(num_heads, head_size)\n",
        "    self.ffn = FeedForwardNetwork(embed_size)\n",
        "    self.ln1 = nn.LayerNorm(embed_size)\n",
        "    self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.self_attn(self.ln1(x))\n",
        "    x = x + self.ffn(self.ln2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "wAxnntcH34Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class LanguageTranslatorEncoder(nn.Module):\n",
        "  def __init__(self, num_heads):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "    self.blocks = nn.Sequential(*[EncoderBlock(embed_size, 4) for _ in range(n_layers)])\n",
        "    self.lm = nn.LayerNorm(embed_size)\n",
        "    self.ll = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, target = None):\n",
        "    B,T = idx.shape\n",
        "    char_embds = self.token_embedding_table(idx)\n",
        "    pos_embds = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    logits = char_embds + pos_embds\n",
        "    logits = self.blocks(logits)\n",
        "    logits = self.lm(logits)\n",
        "    logits = self.ll(logits)\n",
        "    if target is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      target = target.view(B*T)\n",
        "      loss = F.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, loss\n"
      ],
      "metadata": {
        "id": "RlhkQkF442ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embed_size, num_heads):\n",
        "    super().__init__()\n",
        "    head_size = embed_size//num_heads\n",
        "    self.masked_self_attn = MultiheadAttention(num_heads, head_size, masked = True)\n",
        "    self.cross_attn = MultiheadAttention(num_heads, head_size, cross_attention = True)\n",
        "    self.ffn = FeedForwardNetwork(embed_size)\n",
        "    self.ln1 = nn.LayerNorm(embed_size)\n",
        "    self.ln2 = nn.LayerNorm(embed_size)\n",
        "    self.ln3 = nn.LayerNorm(embed_size)\n",
        "    self.ln4 = nn.LayerNorm(embed_size)\n",
        "\n",
        "  def forward(self, x, enc):\n",
        "    x = x + self.masked_self_attn(self.ln1(x))\n",
        "    x = x + self.cross_attn(self.ln2(x), self.ln3(enc))\n",
        "    x = x + self.ffn(self.ln4(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "g5iCDagY2e4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageTranslatorDecoder(nn.Module):\n",
        "  def __init__(self, num_heads):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "    self.blocks = nn.Sequential(*[DecoderBlock(embed_size, 4) for _ in range(n_layers)])\n",
        "    self.lm = nn.LayerNorm(embed_size)\n",
        "    self.ll = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, target = None):\n",
        "    B,T = idx.shape\n",
        "    char_embds = self.token_embedding_table(idx)\n",
        "    pos_embds = self.position_embedding_table(torch.arange(T, device = device))\n",
        "    logits = char_embds + pos_embds\n",
        "    logits = self.blocks(logits)\n",
        "    logits = self.lm(logits)\n",
        "    logits = self.ll(logits)\n",
        "    if target is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      target = target.view(B*T)\n",
        "      loss = F.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens, num_heads):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits,_ = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      soft_max = nn.Softmax(dim = 1)\n",
        "      probab  = soft_max(logits)\n",
        "      predictions = torch.multinomial(probab, 1)\n",
        "      idx = torch.cat((idx, predictions), -1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "oP_BuBtM2iYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSPkyWdXI6JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_E4T9C9g2n-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}